wandb:
  log: True
  entity: "llm-eval"
  project: "jmt-bench"

testmode: false

api: false

model:
  use_wandb_artifacts: false
  artifacts_path: ""
  # ↓ ここをHuggingFace上のモデル名にする。（例: "meta-llama/Llama-2-7b-hf"）
  pretrained_model_name_or_path: "Qwen/Qwen2.5-72B-Instruct"
  trust_remote_code: true
  device_map: "auto"
  load_in_8bit: false
  load_in_4bit: false

generator:
  do_sample: false
  num_beams: 1
  top_p: 1.0
  top_k: 0
  temperature: 0.1
  repetition_penalty: 1.0

tokenizer:
  use_wandb_artifacts: false
  artifacts_path: ""
  pretrained_model_name_or_path: "Qwen/Qwen2.5-72B-Instruct "
  use_fast: false

# jaster等の評価を行わない or 同時に行う設定なら以下を調整
max_seq_length: 1024
dataset_artifact: null
dataset_dir: null
target_dataset: "all"
log_dir: "./logs"
torch_dtype: "bf16"
custom_prompt_template: null
custom_fewshots_template: null

metainfo:
  basemodel_name: "Qwen/Qwen2.5-72B-Instruct " # モデル名を記載
  model_type: "open llm"
  instruction_tuning_method: "None"
  instruction_tuning_data: ["None"]
  num_few_shots: 0
  llm-jp-eval-version: "1.1.0"

# mtbenchに関する設定
mtbench:
  # testmode: true で小さいデータセットテストも可能
  question_artifacts_path: "wandb-japan/llm-leaderboard/mtbench_ja_question:v0"
  referenceanswer_artifacts_path: "wandb-japan/llm-leaderboard/mtbench_ja_referenceanswer:v0"
  judge_prompt_artifacts_path: "wandb-japan/llm-leaderboard/mtbench_ja_prompt:v1"
  bench_name: "japanese_mt_bench"
  model_id: null      # nullにすると内部でhash付きのmodel_idが自動生成される
  question_begin: null
  question_end: null
  max_new_token: 1024
  num_choices: 1
  num_gpus_per_model: 1
  num_gpus_total: 1
  max_gpu_memory: null
  dtype: bfloat16 
  judge_model: "gpt-4o-mini"  # 内部で使用する評価モデル
  mode: "single"
  baseline_model: null
  parallel: 1
  first_n: null
  custom_conv_template: true
  conv_name: "custom"
  conv_system_template: "{system_message}"
  conv_system_message: "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。"
  conv_roles: "('指示', '応答')"
  conv_sep: "\n\n### "
  conv_sep2: ""
  conv_stop_token_ids: "[2]"
  conv_stop_str: "###"
  conv_sep_style: "custom"
  conv_role_message_separator: ":\n"
  conv_role_only_separator: ":\n"
